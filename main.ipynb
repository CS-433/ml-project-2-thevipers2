{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Project 2:\n",
    "# _Autoencoder for mathematical modeling of blood flow in a stenosis context_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are going to analyze data derived from..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goals:\n",
    "1. modeliser mathématiquement l’écoulement sanguin à l’aide de PDE dépendante de 2-3 paramètres physique\n",
    "2. simuler l’écoulement par un code d’éléments fini ou similaire\n",
    "3. générer beaucoup de solutions avec une grande nombre de paramètres différents. (Les 2-3, pris de façon aléatoire)\n",
    "4. a. utiliser les solutions numérique pour établir un auto encoder qui au milieu n’ai que 2-5 hyper-paramètres libres  \n",
    "b. quel erreur on obtient ? Est-ce possible de réduire le nombre d’hyper-paramètres ?\n",
    "5. étudier s’il y a une rélation entre les 2-5 hyper-paramètre et les paramètres physique  \n",
    "a. à l’aide de statistiques\n",
    "5. b. à l’aide d’un DNN (différent de 4a)\n",
    "6. (optionnel) faire un DNN entre l’input de 4a et output les paramêtres physique. Et/ou l’inverse.\n",
    "7. Discussion et conclusions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbreviations used:\n",
    "- $N_u$ = total number of spatial points per simulation\n",
    "- $N_t$ = total number of time steps per simulation\n",
    "- $N_s$ = total number of simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Data exploration](#data_exploration) \n",
    "- [Imports](#1imports)\n",
    "- [Pathways](#1pathways)\n",
    "- [Loading](#1load)\n",
    "- [Exploration](#1exploration)\n",
    "\n",
    "[2. Data preprocessing](#preprocessing)\n",
    "\n",
    "[3. Autoencode](#classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data exploration  <a name=\"data_exploration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports  <a name=\"1imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helpers import *\n",
    "from preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pathways <a name=\"1pathways\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'\n",
    "DATA_Ux_PATH = DATA_PATH + 'u1_very_small.csv'\n",
    "DATA_Uy_PATH = DATA_PATH + 'u2_very_small.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading <a name=\"1load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data which consists of 2 matrices Ux, Uy denoting the x and y coordinates of the speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading\n",
    "Ux_pd = pd.read_csv(DATA_Ux_PATH, header=None)\n",
    "Uy_pd = pd.read_csv(DATA_Uy_PATH, header=None)\n",
    "# Converting from dataframe to numpy\n",
    "Ux = Ux_pd.to_numpy()\n",
    "Uy = Uy_pd.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration <a name=\"1exploration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at our imported data sets. The columns represent the time steps and the rows represent a point of our blood vessel mesh and each 5509 row a new simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our input array Ux is of shape: (137725, 110)\n",
      "Our input array Uy is of shape: (143234, 110)\n",
      "Printing a row of Ux: \n",
      " [0.07843621 0.31118352 0.68245424 1.15863002 1.69435217 2.2377946\n",
      " 2.73607737 3.14059293 3.41183529 3.52330289 3.63726074 3.7439734\n",
      " 3.84549407 3.94281205 4.03640323 4.12650964 4.2132592  4.29672103\n",
      " 4.37693287 4.45391539 4.52767992 4.59823263 4.6655769  4.72971455\n",
      " 4.79064658 4.84837353 4.90289569 4.95421322 5.00232621 5.04723472\n",
      " 5.08893877 5.12743839 5.16273359 5.19482437 5.22371075 5.24939272\n",
      " 5.2718703  5.29114348 5.30721227 5.32007667 5.32973668 5.33619229\n",
      " 5.33944353 5.33997115 5.33936904 5.33787084 5.33552701 5.33235975\n",
      " 5.32837974 5.3235924  5.31800058 5.31160582 5.30440894 5.29641041\n",
      " 5.28761047 5.27800926 5.26760686 5.25640331 5.24439864 5.23159284\n",
      " 5.21798594 5.20357794 5.18836883 5.17235862 5.15554731 5.13793491\n",
      " 5.1195214  5.1003068  5.0802911  5.05947431 5.03785642 5.01543743\n",
      " 4.99221734 4.96819616 4.94337388 4.9177505  4.89132603 4.86410046\n",
      " 4.8360738  4.80724604 4.77761718 4.74718722 4.71595617 4.68392403\n",
      " 4.65109078 4.61745644 4.583021   4.54778447 4.51174684 4.47490812\n",
      " 4.43726829 4.39882737 4.35958536 4.31954225 4.27869804 4.23705274\n",
      " 4.19460633 4.15135884 4.10731024 4.06246055 4.01680977 3.97035788\n",
      " 3.9231049  3.87505083 3.82619566 3.77653939 3.72608202 3.67482356\n",
      " 3.622764   3.56990335] \n",
      "\n",
      "Printing a row of Uy: \n",
      " [0.07843621 0.31118352 0.68245424 1.15863002 1.69435217 2.2377946\n",
      " 2.73607737 3.14059293 3.41183529 3.52330289 3.63726074 3.7439734\n",
      " 3.84549407 3.94281205 4.03640323 4.12650964 4.2132592  4.29672103\n",
      " 4.37693287 4.45391539 4.52767992 4.59823263 4.6655769  4.72971455\n",
      " 4.79064658 4.84837353 4.90289569 4.95421322 5.00232621 5.04723472\n",
      " 5.08893877 5.12743839 5.16273359 5.19482437 5.22371075 5.24939272\n",
      " 5.2718703  5.29114348 5.30721227 5.32007667 5.32973668 5.33619229\n",
      " 5.33944353 5.33997115 5.33936904 5.33787084 5.33552701 5.33235975\n",
      " 5.32837974 5.3235924  5.31800058 5.31160582 5.30440894 5.29641041\n",
      " 5.28761047 5.27800926 5.26760686 5.25640331 5.24439864 5.23159284\n",
      " 5.21798594 5.20357794 5.18836883 5.17235862 5.15554731 5.13793491\n",
      " 5.1195214  5.1003068  5.0802911  5.05947431 5.03785642 5.01543743\n",
      " 4.99221734 4.96819616 4.94337388 4.9177505  4.89132603 4.86410046\n",
      " 4.8360738  4.80724604 4.77761718 4.74718722 4.71595617 4.68392403\n",
      " 4.65109078 4.61745644 4.583021   4.54778447 4.51174684 4.47490812\n",
      " 4.43726829 4.39882737 4.35958536 4.31954225 4.27869804 4.23705274\n",
      " 4.19460633 4.15135884 4.10731024 4.06246055 4.01680977 3.97035788\n",
      " 3.9231049  3.87505083 3.82619566 3.77653939 3.72608202 3.67482356\n",
      " 3.622764   3.56990335] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Our input array Ux is of shape:\",Ux.shape)\n",
    "print(\"Our input array Uy is of shape:\",Uy.shape)\n",
    "print(\"Printing a row of Ux:\", '\\n', Ux[10,:], '\\n')\n",
    "print(\"Printing a row of Uy:\", '\\n', Uy[10,:], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the analysis, we need to figure out the number of simulation step. As we know that we previsouly generated 25 simulations on Matlab and all the new simulations are appended row-wise, we can deduce it with the following computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have run 137725 simulations with a step of 5509.0\n"
     ]
    }
   ],
   "source": [
    "size = Ux.shape[0]/25\n",
    "print(\"We have run\", Ux.shape[0],\"simulations with a step of\",size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing  <a name=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have any NaN or None values in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(np.isnan(Ux)))\n",
    "print(np.count_nonzero(np.isnan(Uy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there are no None values so we can start direclty preprocess our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADDITIONAL IDEAS FOR PREPROCESSING\n",
    "- remove columns with 0 std dev?\n",
    "- standardization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample our data points into the following ratios : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13750, 110) (13750, 110)\n"
     ]
    }
   ],
   "source": [
    "ratio_pts = 0.1\n",
    "ratio_time = 1\n",
    "\n",
    "new_Ux, new_Uy, new_inds = sample(Ux, Uy, ratio_pts, ratio_time)\n",
    "print(new_Ux.shape, new_Uy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the sampling we got from  5509  positions to  550  positions\n",
      "With the sampling we got from  110  time steps to  110  time steps\n"
     ]
    }
   ],
   "source": [
    "new_Nu, new_Nt = get_Nu_Nt_sampled(Ux, new_Ux)\n",
    "\n",
    "print('With the sampling we got from ', 5509, ' positions to ', new_Nu, ' positions')\n",
    "print('With the sampling we got from ', 110, ' time steps to ', new_Nt, ' time steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's flatten our matrices into a single matrix with dimensions $((2 N_u N_t), N_s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4669628  2.95609394 2.9131648  ... 2.16528591 2.02004071 2.93534615]\n",
      " [0.32461604 0.39327742 0.38658752 ... 0.29807351 0.26373287 0.39251852]\n",
      " [2.23956366 2.68549092 2.64593838 ... 1.97405694 1.83301461 2.66750225]\n",
      " ...\n",
      " [3.58186483 4.27567226 4.21769347 ... 3.08544866 2.94060993 4.2379438 ]\n",
      " [0.16419236 0.20173798 0.19768457 ... 0.15929658 0.13202943 0.2026274 ]\n",
      " [1.07779084 1.29176932 1.27313156 ... 0.9428681  0.88227332 1.28281555]]\n"
     ]
    }
   ],
   "source": [
    "print(new_Ux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 121000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_array = flatten(new_Ux, new_Uy, ratio_pts)\n",
    "flattened_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a sanity check that the dimension of 1 datapoint is indeed ${2 * new_{N_t} * new_{N_u})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121000\n"
     ]
    }
   ],
   "source": [
    "assert flattened_array.shape[1] == 2*new_Nt*new_Nu\n",
    "\n",
    "print(2*new_Nt*new_Nu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Auto-encoder  <a name=\"autoencoder\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data set into a training a testing set to be able to evaluate our autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(495, 121000)\n",
      "(55, 121000)\n"
     ]
    }
   ],
   "source": [
    "# Set seed \n",
    "seed = 123\n",
    "flattened_array_train, flattened_array_test = train_test_split(flattened_array, test_size=0.1, random_state=seed)\n",
    "y_train, y_test = flattened_array_train, flattened_array_test\n",
    "\n",
    "print(flattened_array_train.shape)\n",
    "print(flattened_array_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/100, loss = 10.052877\n",
      "epoch : 2/100, loss = 9.828532\n",
      "epoch : 3/100, loss = 9.584318\n",
      "epoch : 4/100, loss = 9.310137\n",
      "epoch : 5/100, loss = 9.010415\n",
      "epoch : 6/100, loss = 8.696991\n",
      "epoch : 7/100, loss = 8.373413\n",
      "epoch : 8/100, loss = 8.052685\n",
      "epoch : 9/100, loss = 7.776497\n",
      "epoch : 10/100, loss = 7.544547\n",
      "epoch : 11/100, loss = 7.311075\n",
      "epoch : 12/100, loss = 7.057894\n",
      "epoch : 13/100, loss = 6.775781\n",
      "epoch : 14/100, loss = 6.502980\n",
      "epoch : 15/100, loss = 6.245518\n",
      "epoch : 16/100, loss = 6.058895\n",
      "epoch : 17/100, loss = 5.908620\n",
      "epoch : 18/100, loss = 5.778327\n",
      "epoch : 19/100, loss = 5.649867\n",
      "epoch : 20/100, loss = 5.510818\n",
      "epoch : 21/100, loss = 5.285861\n",
      "epoch : 22/100, loss = 4.826629\n",
      "epoch : 23/100, loss = 3.983457\n",
      "epoch : 24/100, loss = 2.769870\n",
      "epoch : 25/100, loss = 1.615822\n",
      "epoch : 26/100, loss = 0.999115\n",
      "epoch : 27/100, loss = 0.902560\n",
      "epoch : 28/100, loss = 0.901818\n",
      "epoch : 29/100, loss = 0.901195\n",
      "epoch : 30/100, loss = 0.900783\n",
      "epoch : 31/100, loss = 0.900526\n",
      "epoch : 32/100, loss = 0.911643\n",
      "epoch : 33/100, loss = 0.912822\n",
      "epoch : 34/100, loss = 0.912268\n",
      "epoch : 35/100, loss = 0.910868\n",
      "epoch : 36/100, loss = 0.909169\n",
      "epoch : 37/100, loss = 0.907545\n",
      "epoch : 38/100, loss = 0.905711\n",
      "epoch : 39/100, loss = 0.903833\n",
      "epoch : 40/100, loss = 0.901600\n",
      "epoch : 41/100, loss = 0.900536\n",
      "epoch : 42/100, loss = 0.900146\n",
      "epoch : 43/100, loss = 0.900126\n",
      "epoch : 44/100, loss = 0.900107\n",
      "epoch : 45/100, loss = 0.900112\n",
      "epoch : 46/100, loss = 0.900116\n",
      "epoch : 47/100, loss = 0.900118\n",
      "epoch : 48/100, loss = 0.900119\n",
      "epoch : 49/100, loss = 0.900120\n",
      "epoch : 50/100, loss = 0.900120\n",
      "epoch : 51/100, loss = 0.900121\n",
      "epoch : 52/100, loss = 0.900121\n",
      "epoch : 53/100, loss = 0.900121\n",
      "epoch : 54/100, loss = 0.900122\n",
      "epoch : 55/100, loss = 0.900122\n",
      "epoch : 56/100, loss = 0.900122\n",
      "epoch : 57/100, loss = 0.900121\n",
      "epoch : 58/100, loss = 0.900122\n",
      "epoch : 59/100, loss = 0.900122\n",
      "epoch : 60/100, loss = 0.900121\n",
      "epoch : 61/100, loss = 0.900122\n",
      "epoch : 62/100, loss = 0.900122\n",
      "epoch : 63/100, loss = 0.900122\n",
      "epoch : 64/100, loss = 0.900122\n",
      "epoch : 65/100, loss = 0.900122\n",
      "epoch : 66/100, loss = 0.900122\n",
      "epoch : 67/100, loss = 0.900122\n",
      "epoch : 68/100, loss = 0.900122\n",
      "epoch : 69/100, loss = 0.900122\n",
      "epoch : 70/100, loss = 0.900122\n",
      "epoch : 71/100, loss = 0.900122\n",
      "epoch : 72/100, loss = 0.900122\n",
      "epoch : 73/100, loss = 0.900122\n",
      "epoch : 74/100, loss = 0.900122\n",
      "epoch : 75/100, loss = 0.900122\n",
      "epoch : 76/100, loss = 0.900122\n",
      "epoch : 77/100, loss = 0.900121\n",
      "epoch : 78/100, loss = 0.900122\n",
      "epoch : 79/100, loss = 0.900122\n",
      "epoch : 80/100, loss = 0.900122\n",
      "epoch : 81/100, loss = 10.393456\n",
      "epoch : 82/100, loss = 10.393380\n",
      "epoch : 83/100, loss = 10.393312\n",
      "epoch : 84/100, loss = 10.393431\n",
      "epoch : 85/100, loss = 10.393406\n",
      "epoch : 86/100, loss = 10.393482\n",
      "epoch : 87/100, loss = 10.393086\n",
      "epoch : 88/100, loss = 10.297988\n",
      "epoch : 89/100, loss = 10.338330\n",
      "epoch : 90/100, loss = 10.279267\n",
      "epoch : 91/100, loss = 10.262911\n",
      "epoch : 92/100, loss = 10.255606\n",
      "epoch : 93/100, loss = 10.251578\n",
      "epoch : 94/100, loss = 10.245506\n",
      "epoch : 95/100, loss = 10.236422\n",
      "epoch : 96/100, loss = 10.213610\n",
      "epoch : 97/100, loss = 10.165546\n",
      "epoch : 98/100, loss = 10.136346\n",
      "epoch : 99/100, loss = 10.092356\n",
      "epoch : 100/100, loss = 10.072194\n"
     ]
    }
   ],
   "source": [
    "from autoencoder import *\n",
    "train(flattened_array_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Passer en 2D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyper-parameters and physics parameters relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discussion & conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
